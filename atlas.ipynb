{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b9435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32d4620c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati caricati da file numpy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Caricamento dei dati ===\n",
    "signal_path = \"data/atlas/signal_clusters.npy\"\n",
    "noise_path = \"data/atlas/noise_clusters.npy\"\n",
    "\n",
    "signal_clusters = np.load(signal_path, allow_pickle=True).tolist()\n",
    "noise_clusters = np.load(noise_path, allow_pickle=True).tolist()\n",
    "print(\"Dati caricati da file numpy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bea7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Funzione per estrazione delle feature ===\n",
    "def extract_features(c):\n",
    "    total_charge = [np.sum(cluster) for cluster in c[\"charge\"]]\n",
    "    x = c[\"localPosX\"]\n",
    "    mean_time = [np.average(t, weights=q) for t, q in zip(c[\"stripTimes\"], c[\"charge\"])]\n",
    "    n_strips = [len(cluster) for cluster in c[\"charge\"]]\n",
    "    features = np.stack([total_charge, x, mean_time, n_strips], axis=1)\n",
    "\n",
    "    global_x = c[\"globalPosX\"]\n",
    "    global_y = c[\"globalPosY\"]\n",
    "    global_z = c[\"globalPosZ\"]\n",
    "    global_positions = np.stack([global_x, global_y, global_z], axis=1)\n",
    "\n",
    "    return features, global_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdf2652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di eventi/grafi creati: 20000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Creazione dei grafi per ogni evento ===\n",
    "event_graphs = []\n",
    "k = 4  # numero di vicini per il grafo\n",
    "\n",
    "def create_graphs(clusters, label):\n",
    "    for c in clusters:\n",
    "        features, global_positions = extract_features(c)\n",
    "        if len(features) < 2:\n",
    "            continue\n",
    "        x = torch.tensor(features, dtype=torch.float)\n",
    "        y = torch.tensor(np.full(features.shape[0], label), dtype=torch.long)\n",
    "        pos = torch.tensor(global_positions, dtype=torch.float)\n",
    "        coords = global_positions\n",
    "        N = x.shape[0]\n",
    "        nbrs = NearestNeighbors(n_neighbors=min(k+1, N), algorithm='ball_tree').fit(coords)\n",
    "        _, indices = nbrs.kneighbors(coords)\n",
    "        edge_index = []\n",
    "        for idx, neighbors in enumerate(indices):\n",
    "            for n in neighbors[1:]:\n",
    "                edge_index.append([idx, n])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        event_graphs.append(Data(x=x, edge_index=edge_index, y=y, pos=pos))\n",
    "\n",
    "create_graphs(signal_clusters, label=1)\n",
    "create_graphs(noise_clusters, label=0)\n",
    "\n",
    "print(f\"Numero di eventi/grafi creati: {len(event_graphs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "211bc9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 12000 eventi, Validation set: 4000 eventi, Test set: 4000 eventi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Divisione train/val/test ===\n",
    "train_graphs, temp_graphs = train_test_split(event_graphs, test_size=0.4, random_state=42)\n",
    "val_graphs, test_graphs = train_test_split(temp_graphs, test_size=0.5, random_state=42)\n",
    "print(f\"Train set: {len(train_graphs)} eventi, Validation set: {len(val_graphs)} eventi, Test set: {len(test_graphs)} eventi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d39c2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Normalizzazione delle feature ===\n",
    "all_train_features = np.concatenate([data.x.numpy() for data in train_graphs], axis=0)\n",
    "mean = all_train_features.mean(axis=0)\n",
    "std = all_train_features.std(axis=0)\n",
    "\n",
    "def normalize_features(features, mean, std):\n",
    "    return (features - mean) / std\n",
    "\n",
    "for dataset in [train_graphs, val_graphs, test_graphs]:\n",
    "    for data in dataset:\n",
    "        data.x = torch.tensor(normalize_features(data.x.numpy(), mean, std), dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08845aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Modello GNN ===\n",
    "channels = 4  # total_charge, mean_x, mean_time, n_strips\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels=channels, hidden_channels=64, out_channels=1, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.pre_mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.convs = nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.post_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.pre_mlp(x)\n",
    "        for conv in self.convs:\n",
    "            x_res = x\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = self.dropout(x)\n",
    "            x = x + x_res \n",
    "        x = self.post_mlp(x)\n",
    "        return x.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e071649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Funzione di valutazione ===\n",
    "def evaluate_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = torch.sigmoid(model(data))\n",
    "            preds = (out > 0.5).long().cpu().numpy()\n",
    "            labels = data.y.cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    return acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7361019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Inizializzazione e loss ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GNN(hidden_channels=32, out_channels=1, num_layers=4, dropout=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_graphs, batch_size=16)\n",
    "test_loader = DataLoader(test_graphs, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18bc0a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nessun checkpoint caricato.\n"
     ]
    }
   ],
   "source": [
    "# === Funzioni di checkpoint ===\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def save_model(model, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Modello finale salvato in {path}\")\n",
    "\n",
    "def load_model(model, optimizer, checkpoint_path, load_checkpoint=True):\n",
    "    if load_checkpoint and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "        print(f\"Checkpoint caricato da {checkpoint_path}, riprendo da epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"Nessun checkpoint caricato.\")\n",
    "        start_epoch = 0\n",
    "    return model, optimizer, start_epoch\n",
    "\n",
    "checkpoint_path = \"gnn_model/atlas/checkpoint.pt\"\n",
    "model_path = \"gnn_model/atlas/model.pt\"\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, checkpoint_path, load_checkpoint=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60ec89cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train loss: 0.1290 - Validation loss: 0.0831 - Time: 3.81s\n",
      "Epoch 2/20 - Train loss: 0.0824 - Validation loss: 0.0779 - Time: 3.70s\n",
      "Epoch 3/20 - Train loss: 0.0760 - Validation loss: 0.0728 - Time: 3.66s\n",
      "Epoch 4/20 - Train loss: 0.0713 - Validation loss: 0.0696 - Time: 3.71s\n",
      "Epoch 5/20 - Train loss: 0.0689 - Validation loss: 0.0679 - Time: 3.66s\n",
      "Epoch 6/20 - Train loss: 0.0680 - Validation loss: 0.0647 - Time: 3.64s\n",
      "Epoch 7/20 - Train loss: 0.0640 - Validation loss: 0.0698 - Time: 3.63s\n",
      "Epoch 8/20 - Train loss: 0.0647 - Validation loss: 0.0658 - Time: 3.68s\n",
      "Epoch 9/20 - Train loss: 0.0632 - Validation loss: 0.0632 - Time: 3.63s\n",
      "Epoch 10/20 - Train loss: 0.0619 - Validation loss: 0.0634 - Time: 3.59s\n",
      "Epoch 11/20 - Train loss: 0.0631 - Validation loss: 0.0661 - Time: 3.65s\n",
      "Epoch 12/20 - Train loss: 0.0610 - Validation loss: 0.0655 - Time: 3.72s\n",
      "Epoch 13/20 - Train loss: 0.0593 - Validation loss: 0.0618 - Time: 3.74s\n",
      "Epoch 14/20 - Train loss: 0.0604 - Validation loss: 0.0629 - Time: 3.73s\n",
      "Epoch 15/20 - Train loss: 0.0583 - Validation loss: 0.0640 - Time: 3.76s\n",
      "Epoch 16/20 - Train loss: 0.0583 - Validation loss: 0.0609 - Time: 3.77s\n",
      "Epoch 17/20 - Train loss: 0.0580 - Validation loss: 0.0597 - Time: 3.74s\n",
      "Epoch 18/20 - Train loss: 0.0585 - Validation loss: 0.0606 - Time: 3.77s\n",
      "Epoch 19/20 - Train loss: 0.0575 - Validation loss: 0.0600 - Time: 3.70s\n",
      "Epoch 20/20 - Train loss: 0.0571 - Validation loss: 0.0583 - Time: 3.70s\n",
      "Training completo.\n",
      "Modello finale salvato in gnn_model/atlas/model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Training ===\n",
    "epochs = 20\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = data.y.float().to(device)\n",
    "        loss = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            target = data.y.float().to(device)\n",
    "            loss = loss_fn(out, target)\n",
    "            val_loss += loss.item() * data.num_graphs\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train loss: {avg_train_loss:.4f} - Validation loss: {avg_val_loss:.4f} - Time: {elapsed:.2f}s\")\n",
    "    save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
    "\n",
    "print(\"Training completo.\")\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5823eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val metrics - Acc: 0.981 Prec: 0.923 Rec: 0.950 F1: 0.936\n",
      "Test metrics - Acc: 0.982 Prec: 0.932 Rec: 0.960 F1: 0.946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Metriche ===\n",
    "val_acc, val_prec, val_rec, val_f1 = evaluate_metrics(model, val_loader, device)\n",
    "print(f\"Val metrics - Acc: {val_acc:.3f} Prec: {val_prec:.3f} Rec: {val_rec:.3f} F1: {val_f1:.3f}\")\n",
    "\n",
    "test_acc, test_prec, test_rec, test_f1 = evaluate_metrics(model, test_loader, device)\n",
    "print(f\"Test metrics - Acc: {test_acc:.3f} Prec: {test_prec:.3f} Rec: {test_rec:.3f} F1: {test_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2ce9abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvati i primi 20 esempi in images/atlas/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Visualizzazione esempi ===\n",
    "output_dir = \"images/atlas\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for j in range(20):\n",
    "    data_es = test_graphs[j]\n",
    "    x = data_es.x.cpu().numpy()\n",
    "    labels_true = data_es.y.cpu().numpy()\n",
    "    pos = data_es.pos.cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = torch.sigmoid(model(data_es.to(device)))\n",
    "        pred_labels = (out.squeeze() > 0.5).cpu().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(pos[labels_true == 0, 0], pos[labels_true == 0, 1], pos[labels_true == 0, 2],\n",
    "               c='gray', s=40, label='Rumore', alpha=0.7)\n",
    "    ax.scatter(pos[labels_true == 1, 0], pos[labels_true == 1, 1], pos[labels_true == 1, 2],\n",
    "               c='orange', s=60, label='Segnale', alpha=0.8)\n",
    "\n",
    "    for i in range(len(pos)):\n",
    "        if pred_labels[i] == 1:\n",
    "            ax.plot([pos[i, 0]], [pos[i, 1]], [pos[i, 2]], marker='o', markersize=18,\n",
    "                    markerfacecolor='none', markeredgecolor='blue', markeredgewidth=2, alpha=0.7)\n",
    "\n",
    "    margin = 0.05\n",
    "    for idx, set_lim in enumerate([ax.set_xlim, ax.set_ylim, ax.set_zlim]):\n",
    "        data = pos[:, idx]\n",
    "        delta = (data.max() - data.min()) * margin\n",
    "        set_lim(data.min() - delta, data.max() + delta)\n",
    "\n",
    "    ax.set_xlabel('Global X')\n",
    "    ax.set_ylabel('Global Y')\n",
    "    ax.set_zlabel('Global Z')\n",
    "    ax.set_title(f'Evento {j}: arancione=vero segnale, grigio=rumore, blu=predetto segnale')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/event_{j:02d}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Salvati i primi 20 esempi in images/atlas/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44050fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
