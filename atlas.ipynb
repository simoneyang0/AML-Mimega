{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b18afaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati caricati da file numpy.\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_file = \"data/atlas/data24_13p6TeV.root\"\n",
    "signal_path = \"data/atlas/signal_clusters.npy\"\n",
    "noise_path = \"data/atlas/noise_clusters.npy\"\n",
    "N_EVENTS = 10000  # Numero di eventi di segnale e rumore da salvare\n",
    "\n",
    "load = True  # True per caricare, False per estrarre nuovi dati\n",
    "\n",
    "if os.path.exists(signal_path) and os.path.exists(noise_path) and load:\n",
    "    signal_clusters = np.load(signal_path, allow_pickle=True).tolist()\n",
    "    noise_clusters = np.load(noise_path, allow_pickle=True).tolist()\n",
    "    print(\"Dati caricati da file numpy.\")\n",
    "else:\n",
    "    file = uproot.open(input_file)\n",
    "    tree = file[\"BasicTesterTree;1\"]\n",
    "\n",
    "    # Estrai tutti i dati necessari in una sola volta per efficienza\n",
    "    mmOnTrack_stripCharges = tree[\"mmOnTrackStripCharges\"].array(library=\"np\")\n",
    "    mmOnTrack_localPosX = tree[\"mmOnTrackLocalPos_x\"].array(library=\"np\")\n",
    "    mmOnTrack_stripTimes = tree[\"mmOnTrackStripDriftTimes\"].array(library=\"np\")\n",
    "    mmOnTrack_stationIndex = tree[\"mmOnTrack_stationIndex\"].array(library=\"np\")\n",
    "    mmOnTrack_stationEta = tree[\"mmOnTrack_stationEta\"].array(library=\"np\")\n",
    "    mmOnTrack_MuonLink = tree[\"mmOnTrack_MuonLink\"].array(library=\"np\")\n",
    "    muons_pt = tree[\"muons_pt\"].array(library=\"np\")\n",
    "    muons_author = tree[\"muons_author\"].array(library=\"np\")\n",
    "\n",
    "    mmPRDRandomSectorDumped = tree[\"mmPRDRandomSectorDumped\"].array(library=\"np\")\n",
    "    PRD_MM_stripCharges = tree[\"PRD_MM_stripCharges\"].array(library=\"np\")\n",
    "    PRD_MM_localPosX = tree[\"PRD_MM_localPosX\"].array(library=\"np\")\n",
    "    PRD_MM_stripTimes = tree[\"PRD_MM_stripTimes\"].array(library=\"np\")\n",
    "    PRD_MM_stationIndex = tree[\"PRD_MM_stationIndex\"].array(library=\"np\")\n",
    "    PRD_MM_stationEta = tree[\"PRD_MM_stationEta\"].array(library=\"np\")\n",
    "\n",
    "    mmOnTrack_globalPosX = tree[\"mmOnTrackGlobalPos_x\"].array(library=\"np\")\n",
    "    mmOnTrack_globalPosY = tree[\"mmOnTrackGlobalPos_y\"].array(library=\"np\")\n",
    "    mmOnTrack_globalPosZ = tree[\"mmOnTrackGlobalPos_z\"].array(library=\"np\")\n",
    "\n",
    "    PRD_MM_globalPosX = tree[\"PRD_MM_globalPosX\"].array(library=\"np\")\n",
    "    PRD_MM_globalPosY = tree[\"PRD_MM_globalPosY\"].array(library=\"np\")\n",
    "    PRD_MM_globalPosZ = tree[\"PRD_MM_globalPosZ\"].array(library=\"np\")\n",
    "\n",
    "    # --- Estrazione eventi di segnale ---\n",
    "    signal_clusters = []\n",
    "    for i in range(len(mmOnTrack_stripCharges)):\n",
    "        charges = mmOnTrack_stripCharges[i]\n",
    "        xs = mmOnTrack_localPosX[i]\n",
    "        times = mmOnTrack_stripTimes[i]\n",
    "        muon_link = mmOnTrack_MuonLink[i] if len(mmOnTrack_MuonLink[i]) > 0 else None\n",
    "        pt = muons_pt[i] if len(muons_pt[i]) > 0 else None\n",
    "        author = muons_author[i] if len(muons_author[i]) > 0 else None\n",
    "        n_strips = [len(cluster) for cluster in charges]\n",
    "        station_index = mmOnTrack_stationIndex[i] if len(mmOnTrack_stationIndex[i]) > 0 else None\n",
    "        station_eta = mmOnTrack_stationEta[i] if len(mmOnTrack_stationEta[i]) > 0 else None\n",
    "        global_x = mmOnTrack_globalPosX[i]\n",
    "        global_y = mmOnTrack_globalPosY[i]\n",
    "        global_z = mmOnTrack_globalPosZ[i]\n",
    "        # Condizione: almeno 4 cluster di muoni\n",
    "        selected = []\n",
    "        if (\n",
    "            pt is not None and author is not None and\n",
    "            station_index is not None and station_eta is not None\n",
    "        ):\n",
    "            for j in range(len(charges)):\n",
    "                if (\n",
    "                    station_index[j] == 55 and\n",
    "                    station_eta[j] == 1 and\n",
    "                    pt[muon_link[j]] >= 15 and\n",
    "                    author[muon_link[j]] == 1\n",
    "                ):\n",
    "                    selected.append(j)\n",
    "        if len(selected) >= 4:\n",
    "            signal_clusters.append({\n",
    "                \"charge\": [charges[j] for j in selected],\n",
    "                \"localPosX\": [xs[j] for j in selected],\n",
    "                \"stripTimes\": [times[j] for j in selected],\n",
    "                \"n_strips\": [n_strips[j] for j in selected],\n",
    "                #\"muons_pt\": [pt[muon_link[j]] for j in selected],\n",
    "                #\"muons_author\": [author[muon_link[j]] for j in selected],\n",
    "                \"globalPosX\": [global_x[j] for j in selected],\n",
    "                \"globalPosY\": [global_y[j] for j in selected],\n",
    "                \"globalPosZ\": [global_z[j] for j in selected]\n",
    "            })\n",
    "        if len(signal_clusters) >= N_EVENTS:\n",
    "            break\n",
    "\n",
    "    print(f\"Eventi di segnale salvati: {len(signal_clusters)}\")\n",
    "\n",
    "    # --- Estrazione eventi di rumore ---\n",
    "    noise_clusters = []\n",
    "    for i in range(len(PRD_MM_stripCharges)):\n",
    "        charges = PRD_MM_stripCharges[i]\n",
    "        xs = PRD_MM_localPosX[i]\n",
    "        times = PRD_MM_stripTimes[i]\n",
    "        n_strips = [len(cluster) for cluster in charges]\n",
    "        random_sector = mmPRDRandomSectorDumped[i] if len(mmPRDRandomSectorDumped[i]) > 0 else None\n",
    "        station_index = PRD_MM_stationIndex[i] if len(PRD_MM_stationIndex[i]) > 0 else None\n",
    "        station_eta = PRD_MM_stationEta[i] if len(PRD_MM_stationEta[i]) > 0 else None\n",
    "        global_x = PRD_MM_globalPosX[i]\n",
    "        global_y = PRD_MM_globalPosY[i]\n",
    "        global_z = PRD_MM_globalPosZ[i]\n",
    "        # Trova gli indici dei cluster con station_index pari e station_eta == 1\n",
    "        selected = []\n",
    "        if (\n",
    "            station_index is not None and station_eta is not None and \n",
    "            random_sector is not None and random_sector % 2 == 0\n",
    "        ):\n",
    "            for j in range(len(charges)):\n",
    "                if station_index[j] == 55 and station_eta[j] == 1:\n",
    "                    selected.append(j)\n",
    "            noise_clusters.append({\n",
    "                \"charge\": [charges[j] for j in selected],\n",
    "                \"localPosX\": [xs[j] for j in selected],\n",
    "                \"stripTimes\": [times[j] for j in selected],\n",
    "                \"n_strips\": [n_strips[j] for j in selected],\n",
    "                \"globalPosX\": [global_x[j] for j in selected],\n",
    "                \"globalPosY\": [global_y[j] for j in selected],\n",
    "                \"globalPosZ\": [global_z[j] for j in selected]\n",
    "            })\n",
    "        if len(noise_clusters) >= N_EVENTS:\n",
    "            break\n",
    "\n",
    "    print(f\"Eventi di rumore salvati: {len(noise_clusters)}\")\n",
    "\n",
    "\n",
    "    file.close()\n",
    "    np.save(signal_path, np.array(signal_clusters, dtype=object))\n",
    "    np.save(noise_path, np.array(noise_clusters, dtype=object))\n",
    "    print(\"Dati estratti e salvati in file numpy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13bb5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di eventi/grafi creati: 18605\n",
      "Train set: 11163 eventi, Validation set: 3721 eventi, Test set: 3721 eventi\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# --- Crea tutti i grafi senza normalizzazione ---\n",
    "event_graphs = []\n",
    "k = 8  # Numero di vicini per ogni nodo\n",
    "\n",
    "def extract_features(c):\n",
    "    total_charge = [np.sum(cluster) for cluster in c[\"charge\"]]\n",
    "    mean_x = [np.mean(cluster) for cluster in c[\"localPosX\"]]\n",
    "    mean_time = [np.mean(cluster) for cluster in c[\"stripTimes\"]]\n",
    "    n_strips = [len(cluster) for cluster in c[\"charge\"]]\n",
    "    features = np.stack([total_charge, mean_x, mean_time, n_strips], axis=1)\n",
    "    global_x = [np.mean(cluster) for cluster in c[\"globalPosX\"]]\n",
    "    global_y = [np.mean(cluster) for cluster in c[\"globalPosY\"]]\n",
    "    global_z = [np.mean(cluster) for cluster in c[\"globalPosZ\"]]\n",
    "    global_positions = np.stack([global_x, global_y, global_z], axis=1)\n",
    "    return features, global_positions\n",
    "\n",
    "for c in signal_clusters:\n",
    "    features, global_positions = extract_features(c)\n",
    "    labels = np.ones(features.shape[0], dtype=np.int64)\n",
    "    if len(features) < 2:\n",
    "        continue\n",
    "    x = torch.tensor(features, dtype=torch.float)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    pos = torch.tensor(global_positions, dtype=torch.float)\n",
    "    N = x.shape[0]\n",
    "    coords = x[:, 1:3].numpy()\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k+1, N), algorithm='ball_tree').fit(coords)\n",
    "    _, indices = nbrs.kneighbors(coords)\n",
    "    edge_index = []\n",
    "    for idx, neighbors in enumerate(indices):\n",
    "        for n in neighbors[1:]:\n",
    "            edge_index.append([idx, n])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    event_graphs.append(Data(x=x, edge_index=edge_index, y=y, pos=pos))\n",
    "\n",
    "for c in noise_clusters:\n",
    "    features, global_positions = extract_features(c)\n",
    "    labels = np.zeros(features.shape[0], dtype=np.int64)\n",
    "    if len(features) < 2:\n",
    "        continue\n",
    "    x = torch.tensor(features, dtype=torch.float)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    pos = torch.tensor(global_positions, dtype=torch.float)\n",
    "    N = x.shape[0]\n",
    "    coords = x[:, 1:3].numpy()\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(k+1, N), algorithm='ball_tree').fit(coords)\n",
    "    _, indices = nbrs.kneighbors(coords)\n",
    "    edge_index = []\n",
    "    for idx, neighbors in enumerate(indices):\n",
    "        for n in neighbors[1:]:\n",
    "            edge_index.append([idx, n])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    event_graphs.append(Data(x=x, edge_index=edge_index, y=y, pos=pos))\n",
    "\n",
    "print(f\"Numero di eventi/grafi creati: {len(event_graphs)}\")\n",
    "\n",
    "# --- Split train/val/test a livello di evento ---\n",
    "train_graphs, temp_graphs = train_test_split(event_graphs, test_size=0.4, random_state=42)\n",
    "val_graphs, test_graphs = train_test_split(temp_graphs, test_size=0.5, random_state=42)\n",
    "print(f\"Train set: {len(train_graphs)} eventi, Validation set: {len(val_graphs)} eventi, Test set: {len(test_graphs)} eventi\")\n",
    "\n",
    "# --- Calcola mean e std SOLO sui dati di train ---\n",
    "all_train_features = []\n",
    "for data in train_graphs:\n",
    "    all_train_features.append(data.x.numpy())\n",
    "all_train_features = np.concatenate(all_train_features, axis=0)\n",
    "mean = all_train_features.mean(axis=0)\n",
    "std = all_train_features.std(axis=0)\n",
    "\n",
    "def normalize_features(features, mean, std):\n",
    "    return (features - mean) / std\n",
    "\n",
    "# --- Normalizza tutti i set con mean/std del train ---\n",
    "for dataset in [train_graphs, val_graphs, test_graphs]:\n",
    "    for data in dataset:\n",
    "        data.x = torch.tensor(normalize_features(data.x.numpy(), mean, std), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a33ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_metrics(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = torch.sigmoid(model(data))\n",
    "            preds = (out > 0.5).long().cpu().numpy()\n",
    "            labels = data.y.cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# --- Modello GNN per classificazione grafo ---\n",
    "channels = 4  # total_charge, mean_x, mean_time, n_strips\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels=channels, hidden_channels=64, out_channels=1, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers-1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = self.dropout(x)\n",
    "        x = self.lin(x)\n",
    "        return x.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d4e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/20 - Train loss: 0.2355 - Validation loss: 0.1439 - Time: 3.56s\n",
      "Epoch 2/20 - Train loss: 0.1416 - Validation loss: 0.1150 - Time: 3.48s\n",
      "Epoch 3/20 - Train loss: 0.1253 - Validation loss: 0.1090 - Time: 3.90s\n",
      "Epoch 4/20 - Train loss: 0.1270 - Validation loss: 0.1059 - Time: 7.07s\n",
      "Epoch 5/20 - Train loss: 0.1250 - Validation loss: 0.1022 - Time: 7.31s\n",
      "Epoch 6/20 - Train loss: 0.1229 - Validation loss: 0.1109 - Time: 7.34s\n",
      "Epoch 7/20 - Train loss: 0.1170 - Validation loss: 0.1003 - Time: 7.20s\n",
      "Epoch 8/20 - Train loss: 0.1143 - Validation loss: 0.1017 - Time: 7.00s\n",
      "Epoch 9/20 - Train loss: 0.1118 - Validation loss: 0.1100 - Time: 7.27s\n",
      "Epoch 10/20 - Train loss: 0.1148 - Validation loss: 0.0968 - Time: 4.36s\n",
      "Epoch 11/20 - Train loss: 0.1099 - Validation loss: 0.1016 - Time: 3.79s\n",
      "Epoch 12/20 - Train loss: 0.1081 - Validation loss: 0.1044 - Time: 3.63s\n",
      "Epoch 13/20 - Train loss: 0.1084 - Validation loss: 0.0993 - Time: 3.42s\n",
      "Epoch 14/20 - Train loss: 0.1106 - Validation loss: 0.0982 - Time: 3.70s\n",
      "Epoch 15/20 - Train loss: 0.1104 - Validation loss: 0.0987 - Time: 3.58s\n",
      "Epoch 16/20 - Train loss: 0.1061 - Validation loss: 0.0997 - Time: 3.78s\n",
      "Epoch 17/20 - Train loss: 0.1051 - Validation loss: 0.0973 - Time: 3.84s\n",
      "Epoch 18/20 - Train loss: 0.1095 - Validation loss: 0.0948 - Time: 6.20s\n",
      "Epoch 19/20 - Train loss: 0.1050 - Validation loss: 0.0994 - Time: 7.41s\n",
      "Epoch 20/20 - Train loss: 0.1036 - Validation loss: 0.0949 - Time: 7.48s\n",
      "Training complete.\n",
      "Test accuracy: 96.89%\n",
      "Val metrics - Acc: 0.970 Prec: 0.895 Rec: 0.921 F1: 0.908\n",
      "Test metrics - Acc: 0.969 Prec: 0.895 Rec: 0.930 F1: 0.912\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- Training e test ---\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_graphs, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_graphs, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = GNN(hidden_channels = 32, out_channels = 1, num_layers = 6, dropout = 0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Calcola il rapporto tra classi\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        target = data.y.float().to(device)\n",
    "        loss = loss_fn(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            target = data.y.float().to(device)\n",
    "            loss = loss_fn(out, target)\n",
    "            val_loss += loss.item() * data.num_graphs\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train loss: {avg_train_loss:.4f} - Validation loss: {avg_val_loss:.4f} - Time: {elapsed:.2f}s\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "# Test accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        out = torch.sigmoid(model(data))\n",
    "        pred = (out > 0.5).long()\n",
    "        correct += (pred == data.y.to(device)).sum().item()\n",
    "        total += data.y.size(0)\n",
    "print(f\"Test accuracy: {correct/total:.2%}\")\n",
    "\n",
    "val_acc, val_prec, val_rec, val_f1 = evaluate_metrics(model, val_loader, device)\n",
    "print(f\"Val metrics - Acc: {val_acc:.3f} Prec: {val_prec:.3f} Rec: {val_rec:.3f} F1: {val_f1:.3f}\")\n",
    "\n",
    "test_acc, test_prec, test_rec, test_f1 = evaluate_metrics(model, test_loader, device)\n",
    "print(f\"Test metrics - Acc: {test_acc:.3f} Prec: {test_prec:.3f} Rec: {test_rec:.3f} F1: {test_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5a8c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvati i primi 20 esempi in images/atlas/\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Crea la cartella se non esiste\n",
    "output_dir = \"images/atlas\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for j in range(20):\n",
    "    data_es = test_graphs[j]\n",
    "    x = data_es.x.cpu().numpy()\n",
    "    labels_true = data_es.y.cpu().numpy()\n",
    "    pos = data_es.pos.cpu().numpy()  # shape: [num_clusters, 3]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = torch.sigmoid(model(data_es.to(device)))\n",
    "        pred_labels = (out.squeeze() > 0.5).cpu().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot rumore (label 0)\n",
    "    mask_noise = labels_true == 0\n",
    "    ax.scatter(pos[mask_noise, 0], pos[mask_noise, 1], pos[mask_noise, 2], c='gray', s=40, label='Rumore', alpha=0.7)\n",
    "\n",
    "    # Plot segnale (label 1)\n",
    "    mask_signal = labels_true == 1\n",
    "    ax.scatter(pos[mask_signal, 0], pos[mask_signal, 1], pos[mask_signal, 2], c='orange', s=60, label='Segnale', alpha=0.8)\n",
    "\n",
    "    # Cerchio blu attorno ai cluster predetti come segnale dal modello\n",
    "    for i in range(len(pos)):\n",
    "        if pred_labels[i] == 1:\n",
    "            ax.plot([pos[i,0]], [pos[i,1]], [pos[i,2]], marker='o', markersize=18, markerfacecolor='none', markeredgecolor='blue', markeredgewidth=2, alpha=0.7)\n",
    "\n",
    "    # Zoom automatico sui dati\n",
    "    margin = 0.05\n",
    "    for idx, set_lim in enumerate([ax.set_xlim, ax.set_ylim, ax.set_zlim]):\n",
    "        data = pos[:, idx]\n",
    "        delta = (data.max() - data.min()) * margin\n",
    "        set_lim(data.min() - delta, data.max() + delta)\n",
    "\n",
    "    ax.set_xlabel('Global X')\n",
    "    ax.set_ylabel('Global Y')\n",
    "    ax.set_zlabel('Global Z')\n",
    "    ax.set_title(f'Evento {j}: arancione=vero segnale, grigio=rumore, blu=predetto segnale')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/event_{j:02d}.png\")\n",
    "    plt.close(fig)\n",
    "print(\"Salvati i primi 20 esempi in images/atlas/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b78102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
